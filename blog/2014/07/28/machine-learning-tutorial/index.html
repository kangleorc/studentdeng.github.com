
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>机器学习(一) 简单的背景介绍、线性回归、梯度下降 - 不会开机的男孩</title>
  <meta name="author" content="studentdeng">

  
  <meta name="description" content="Introduction 机器学习很久之前就已经热得不行了，直到最近这几个星期，自己才打算了解一些这方面的东西。原因大概有这么3点。 自从Andrew Ng 加入我厂之后(虽然和我毛关系也没有)，总觉得还是需要围观一下这个令他兴奋的领域。
在听了IDL的有关手环算法分享后(其实毛也没有听懂), &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://studentdeng.github.com/blog/2014/07/28/machine-learning-tutorial">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="不会开机的男孩" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/jquery.min1.9.1.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner"><hgroup>
  <h1><a href="/">不会开机的男孩</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:studentdeng.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  	<li><a href="/">Blog</a></li>
  	<li><a href="/blog/archives">Archives</a></li>
	 <li><a href="/tags/index.html">Tags</a></li>
	<li><a href="/favorite/index.html">Favorite</a></li>
	<li><a href="/about/index.html">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">机器学习(一) 简单的背景介绍、线性回归、梯度下降</h1>
    
    
      <p class="meta">
        








  


<time datetime="2014-07-28T16:48:00+08:00" pubdate data-updated="true">Jul 28<span>th</span>, 2014</time>
        
         | <a href="#disqus_thread">Comments</a>
        
      </p>
    
  </header>


<div class="entry-content"><h1>Introduction</h1>

<p>机器学习很久之前就已经热得不行了，直到最近这几个星期，自己才打算了解一些这方面的东西。原因大概有这么3点。</p>

<ol>
<li>自从Andrew Ng 加入我厂之后(虽然和我毛关系也没有)，总觉得还是需要围观一下这个令他兴奋的领域。</li>
<li>在听了IDL的有关手环算法分享后(其实毛也没有听懂), 在知道了一大堆的名词如最小二乘、梯度下降、SVM。以及里面很多的线性代数，微积分的概念，让我觉得这是一个很好的回收自己大学时期的沉默成本(微积分、现代是我在学校里面不多的用心学过的课程)的好机会。总之就是对这些很感兴趣。</li>
<li>前一段时间受组里高工分享睡眠算法影响，对这种阅读paper，然后优化算法的过程感到很开心。</li>
</ol>


<p>有了这3条，足够我忙活好几个月了 : )</p>

<h1>Background</h1>

<p>在机器学习中，有2个很大的思路<code>监督学习(supervised learning)</code>和<code>非监督学习(unsupervised learning)</code></p>

<p>监督学习，用通俗的话来说就是<code>你知道问题的答案，需要计算机给出一个更标准的答案</code>。</p>

<p>非监督学习，用通俗的话来说就是<code>物以类聚，人以群分</code>。我们拿到了很多数据，但是不知道问题的答案，希望计算机给我们提供思路。</p>

<p>在生产环境中，往往采用混合模式。比如图片搜索，如何能够查找网页中判断那个图片是老虎，那个是狗。就有2个思路。</p>

<ol>
<li>根据图片周围的文字。</li>
<li>图片的图像数据分析。</li>
</ol>


<p>2个角度相互校验，稳定之后，就可以产生足够的标注信息了。</p>

<h1>线性回归(Linear regression)</h1>

<p>线性回归主要用于手环的里程部分的计算，涉及到更细节的是 最小二乘，梯度下降。这里从先从最简单的一元线性回归开始。</p>

<h2>一元线性回归(Linear regression with one variable)</h2>

<p>Regression Problem : Predict real-valued output</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/8.png' width='' height='' title='1-1 算法运行的过程'><span class='caption-text'>1-1 算法运行的过程</span></span></p>

<p>最关键的在于如何描述hypothesis。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/1.png' width='' height='' title='1-2 一元线性回归中的hypothesis函数'><span class='caption-text'>1-2 一元线性回归中的hypothesis函数</span></span></p>

<p>那么应该如何选取参数呢？直觉告诉我们这个直线需要尽可能的拟合我们的数据集。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/9.png' width='' height='' title='1-3 线性回归的目标函数'><span class='caption-text'>1-3 线性回归的目标函数</span></span></p>

<p>通过下面的cost function 来评估参数的好坏。算法的目标也很清晰，让函数越小越好。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/12.png' width='' height='' title='1-4 cost function'><span class='caption-text'>1-4 cost function</span></span></p>

<p>那个这个cost function 到底是个什么样子呢？</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/2.png' width='' height='' title='1-5 图形化的cost function'><span class='caption-text'>1-5 图形化的cost function</span></span></p>

<p>当然这个图还是看起来比较麻烦，Andrew 用了更为简单绘制的图来表示（有点类似等高线）。
相同的圆圈上，有着相同的cost function value。这里可以看到和上面的图一样，有一个极值。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/7.png' width='' height='' title='1-6 一个比较差的选择'><span class='caption-text'>1-6 一个比较差的选择</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/18.png' width='' height='' title='1-7 一个很接近极值的选择'><span class='caption-text'>1-7 一个很接近极值的选择</span></span></p>

<h1>梯度下降 (Gradient descent)</h1>

<p>梯度下降，不仅仅是用于线性回归，也可以用在其他机器学习的场景下。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/3.png' width='' height='' title='1-8 梯度下降的思路（2个参数的情况）'><span class='caption-text'>1-8 梯度下降的思路（2个参数的情况）</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/10.png' width='' height='' title='1-9 梯度下降函数图形（2个参数的情况）'><span class='caption-text'>1-9 梯度下降函数图形（2个参数的情况）</span></span></p>

<p>我们的目标是寻找这个图形中的最小值，也就是靠近蓝色的地方。直觉告诉我们，我们先随机一个点，然后沿着最大的坡度向下走最后就可以走到一个极值里。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/16.png' width='' height='' title='1-10 一条算法路径，全局最优'><span class='caption-text'>1-10 一条算法路径，全局最优</span></span></p>

<p>这个算法也有问题，随着第一个点的位置不同，我们可能找到一个局部最优的解，而不是全局最优。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/14.png' width='' height='' title='1-11 另一条算法路径，局部最优'><span class='caption-text'>1-11 另一条算法路径，局部最优</span></span></p>

<p>好在在很多实际问题中，我们遇到的情况要好很多，往往<strong>只有一个极值</strong>。</p>

<p>那么梯度下降的算法就可以简单的描述出来，分别计算2个维度的偏导数，直到函数收敛</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/5.png' width='' height='' title='1-12'><span class='caption-text'>1-12</span></span></p>

<p>通过分别计算偏导数,a 为learning rate，决定每一步的步长，太小函数收敛很慢，太大则可能无法找到极值，甚至函数无法收敛。</p>

<p>这里Andrew 着重指出了一个叫做同步更新的概念</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/11.png' width='' height='' title='1-13'><span class='caption-text'>1-13</span></span></p>

<p>如果不同步更新，最后也可以得到极致，但是Andrew 更推荐计算完成所有的参数之后，再一起同步更新。</p>

<h2>梯度下降和一元线性回归</h2>

<p>将图1-4分别偏导后</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/15.png' width='' height='' title='1-14 算法公式'><span class='caption-text'>1-14 算法公式</span></span></p>

<h2>其他</h2>

<ol>
<li>根据上面的算法，如果我们的cost function 在一些地方不可导，那算法不就没法继续了？</li>
<li>有其他的方法，可以不去循环计算而是直接根据工具计算</li>
</ol>


<h2>梯度下降和一般化的线性回归</h2>

<p>很多时候我们不仅仅满足2个参数，决定事情的因素很多，我们需要更一般化的公式。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/4.png' width='' height='' title='1-15'><span class='caption-text'>1-15</span></span></p>

<p>算法</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/19.png' width='' height='' title='1-16'><span class='caption-text'>1-16</span></span></p>

<p>分别求偏导后</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/17.png' width='' height='' title='1-17'><span class='caption-text'>1-17</span></span></p>

<h1>梯度下降生产环境中的一些技巧</h1>

<h2>Feature Scaling</h2>

<p>思路: 希望所有的feature在相同或是类似的范围之内，这样梯度下降会更快收敛。</p>

<p>下图是feature的范围不在一起的运算过程，可以看出来不是圆形，2个维度调整的步长不一样，导致很多反复</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/21.png' width='' height='' title='1-18 红色箭头表示算法的一次迭代'><span class='caption-text'>1-18 红色箭头表示算法的一次迭代</span></span></p>

<p>下图则是调整过的feature，好了很多</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/23.png' width='' height='' title='1-19 红色箭头表示算法的一次迭代'><span class='caption-text'>1-19 红色箭头表示算法的一次迭代</span></span></p>

<p>更一般的，Andrew 推荐每一个feature放在[-1, 1]区间范围内</p>

<h2>Learning Rate</h2>

<p>说到Learning Rate 就不能不提收敛(convergence)。一般应该定义多大的阀值来判断是否收敛呢？</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/6.png' width='' height='' title='1-20 Andrew 并不推荐使用一个阀值来判断是否收敛'><span class='caption-text'>1-20 Andrew 并不推荐使用一个阀值来判断是否收敛</span></span></p>

<p>Andrew 更推荐用图表的形式，因为这个不仅仅可以看到是否马上收敛，而且还能看到算法是否运行正常，是不是一些参数的问题，导致算法无法收敛。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/20.png' width='' height='' title='1-21'><span class='caption-text'>1-21</span></span></p>

<p>下图是2个出了问题的J函数，通常来说是Learning Rate 过大。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/25.png' width='' height='' title='1-22 一些过大的Learning Rate 导致的图形'><span class='caption-text'>1-22 一些过大的Learning Rate 导致的图形</span></span></p>

<p>最后Andrew 还提供了一些practice的Learning Rate 选取方法，比如一些0.001, 0.003, 0.01, 0.03, 0.1, &#8230;</p>

<h2>参考</h2>

<p><a href="https://class.coursera.org/ml-006">Coursera 《Machine Learning》 Stanford Andrew Ng</a></p>
</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">studentdeng</span></span>

      








  


<time datetime="2014-07-28T16:48:00+08:00" pubdate data-updated="true">Jul 28<span>th</span>, 2014</time>
      

<span class="categories">
  
    <a class='category' href='/blog/categories/algorithms/'>algorithms</a>, <a class='category' href='/blog/categories/ji-qi-xue-xi/'>机器学习</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2014/06/24/core-animation/" title="Previous Post: Core Animation基本概念和Additive Animation">&laquo; Core Animation基本概念和Additive Animation</a>
      
      
        <a class="basic-alignment right" href="/blog/2014/08/16/qinshihuang/" title="Next Post: 《中国古代历史与人物——秦始皇》笔记">《中国古代历史与人物——秦始皇》笔记 &raquo;</a>
      
    </p>
  </footer>
</article>

  <section>
    <h1>Comments</h1>
    <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
  </section>


</div>

<aside class="sidebar">
  
    
  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 - studentdeng -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> | Themed with <a href="https://github.com/lucaslew/whitespace">Whitespace</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'studentdeng';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://studentdeng.github.com/blog/2014/07/28/machine-learning-tutorial/';
        var disqus_url = 'http://studentdeng.github.com/blog/2014/07/28/machine-learning-tutorial/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>








</body>
</html>
