<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: algorithms | 不会开机的男孩]]></title>
  <link href="http://studentdeng.github.com/blog/categories/algorithms/atom.xml" rel="self"/>
  <link href="http://studentdeng.github.com/"/>
  <updated>2014-11-06T17:14:53+08:00</updated>
  <id>http://studentdeng.github.com/</id>
  <author>
    <name><![CDATA[studentdeng]]></name>
    <email><![CDATA[studentdeng@hotmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[机器学习(二) 线性回归、梯度下降实现]]></title>
    <link href="http://studentdeng.github.com/blog/2014/08/24/machine-learning-2/"/>
    <updated>2014-08-24T20:52:00+08:00</updated>
    <id>http://studentdeng.github.com/blog/2014/08/24/machine-learning-2</id>
    <content type="html"><![CDATA[<p>了解一个算法最好的方法就是实现它，不过在开始实现算法之前，有一些额外的概念需要理解。</p>

<h1>Vectorization</h1>

<p>这是<a href="http://studentdeng.github.io/blog/2014/07/28/machine-learning-tutorial/">上一篇</a>提到的hypothesis的计算公式。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_1.png' width='' height='' title=''><span class='caption-text'></span></span></p>

<p>当计算这个表达式值的时候，往往第一个感觉是写一个for loop 然后累加求和</p>

<pre><code>prediction = 0;
for (int i = 0; i &lt; n; ++i) {
    prediction += theta[j] * x[j];
}
</code></pre>

<p>但是在machine learning中更倾向于使用矩阵的方式。
比如同样的公式，会看成矩阵相乘。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_3.png' width='' height='' title=''><span class='caption-text'></span></span></p>

<p>其中theta和X分别是</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_2.png' width='' height='' title=''><span class='caption-text'></span></span></p>

<p>这里通过矩阵或是向量来代替之前的loop。</p>

<p>这是<a href="http://studentdeng.github.io/blog/2014/07/28/machine-learning-tutorial/">上一篇</a>提到的算法</p>

<p><img src="http://studentdeng.github.io/images/ml/4.png" alt="image" /></p>

<p>计算function J如果用octave来实现则是这个样子</p>

<pre><code>function J = computeCost(X, y, theta)
%COMPUTECOST Compute cost for linear regression
%   J = COMPUTECOST(X, y, theta) computes the cost of using theta as the
%   parameter for linear regression to fit the data points in X and y

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta
%               You should set J to the cost.


t = (X * theta) - y;
J = (sum(t .* t)) / (2 * m);

% =========================================================================

end
</code></pre>

<p><img src="http://studentdeng.github.io/images/ml/19.png" alt="image" /></p>

<p>而求偏导数迭代更新theta的代码则是这个样子</p>

<pre><code>function [theta, J_history] = gradientDescent(X, y, theta, alpha, num_iters)
%GRADIENTDESCENT Performs gradient descent to learn theta
%   theta = GRADIENTDESENT(X, y, theta, alpha, num_iters) updates theta by 
%   taking num_iters gradient steps with learning rate alpha

% Initialize some useful values
m = length(y); % number of training examples
J_history = zeros(num_iters, 1);

for iter = 1:num_iters

    % ====================== YOUR CODE HERE ======================
    % Instructions: Perform a single gradient step on the parameter vector
    %               theta. 
    %
    % Hint: While debugging, it can be useful to print out the values
    %       of the cost function (computeCost) and gradient here.
    %


    s = sum(bsxfun(@times, X * theta - y, X));
    theta = theta - (alpha / m) * s';

    % ============================================================

    % Save the cost J in every iteration    
    J_history(iter) = computeCost(X, y, theta);

end
</code></pre>

<p>上面的2部分代码如果做一些合并分别可以简化成1行代码。说到这里自己还是相当羞愧的。今天早上花了3个小时才搞定这2行代码...主要时间花在了
2个地方。</p>

<ol>
<li>算好theta去predict的上面，和normal equations的方式计算的答案总是对不上，不得不怀疑人生了。。。后面才发现是因为函数没有完全收敛，在调整learning rate之后误差明显变小了。</li>
<li>让大脑适应矩阵还是有点难，很多东西看上去很简单，反应很长时间，不过后面会好一些。</li>
</ol>


<h1>为什么用矩阵</h1>

<p>在费了老半天力气搞定Vectorization的转变之后，不得不想想为什么要用这个方式做。obviously有2个好处，Andrew课上也提到了好多次。</p>

<ol>
<li>增加一个feature很简单，只要把输入增加一列就好，而算法不需要改动。</li>
<li>矩阵的运算更容易优化，性能比循环更快。实际我们往往处理上百万个Example和N多的features</li>
</ol>


<p>第一个很好理解，而且把循环的一大堆代码写成一行，显得逼格很高。
第二个会比较麻烦，涉及到了并行计算优化。</p>

<h1>其他</h1>

<p>在之前的算法中，我们看到了每一次调整theta都需要iterate整个所有的example，但实际中往往需要处理上百万个examples，而这样的iteration显然是不能接受的。实际上会随机选取一部分examples然后去迭代theta，最后得到一个较为可靠的theta向量。</p>

<p>最后附上Andrew作业的图片，虽然Andrew 不希望把答案放在网上或是论坛什么的，不过我觉得都过去2年多了,应该没关系了。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_4.png' width='' height='' title='最后的预测效果图'><span class='caption-text'>最后的预测效果图</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_5.png' width='' height='' title='cost function &amp; theta'><span class='caption-text'>cost function &amp; theta</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_6.png' width='' height='' title='cost function &amp; theta 等高线'><span class='caption-text'>cost function &amp; theta 等高线</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml2_7.png' width='' height='' title='learning rate'><span class='caption-text'>learning rate</span></span></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[机器学习(一) 简单的背景介绍、线性回归、梯度下降]]></title>
    <link href="http://studentdeng.github.com/blog/2014/07/28/machine-learning-tutorial/"/>
    <updated>2014-07-28T16:48:00+08:00</updated>
    <id>http://studentdeng.github.com/blog/2014/07/28/machine-learning-tutorial</id>
    <content type="html"><![CDATA[<h1>Introduction</h1>

<p>机器学习很久之前就已经热得不行了，直到最近这几个星期，自己才打算了解一些这方面的东西。原因大概有这么3点。</p>

<ol>
<li>自从Andrew Ng 加入我厂之后(虽然和我毛关系也没有)，总觉得还是需要围观一下这个令他兴奋的领域。</li>
<li>在听了IDL的有关手环算法分享后(其实毛也没有听懂), 在知道了一大堆的名词如最小二乘、梯度下降、SVM。以及里面很多的线性代数，微积分的概念，让我觉得这是一个很好的回收自己大学时期的沉默成本(微积分、现代是我在学校里面不多的用心学过的课程)的好机会。总之就是对这些很感兴趣。</li>
<li>前一段时间受组里高工分享睡眠算法影响，对这种阅读paper，然后优化算法的过程感到很开心。</li>
</ol>


<p>有了这3条，足够我忙活好几个月了 : )</p>

<h1>Background</h1>

<p>在机器学习中，有2个很大的思路<code>监督学习(supervised learning)</code>和<code>非监督学习(unsupervised learning)</code></p>

<p>监督学习，用通俗的话来说就是<code>你知道问题的答案，需要计算机给出一个更标准的答案</code>。</p>

<p>非监督学习，用通俗的话来说就是<code>物以类聚，人以群分</code>。我们拿到了很多数据，但是不知道问题的答案，希望计算机给我们提供思路。</p>

<p>在生产环境中，往往采用混合模式。比如图片搜索，如何能够查找网页中判断那个图片是老虎，那个是狗。就有2个思路。</p>

<ol>
<li>根据图片周围的文字。</li>
<li>图片的图像数据分析。</li>
</ol>


<p>2个角度相互校验，稳定之后，就可以产生足够的标注信息了。</p>

<h1>线性回归(Linear regression)</h1>

<p>线性回归主要用于手环的里程部分的计算，涉及到更细节的是 最小二乘，梯度下降。这里从先从最简单的一元线性回归开始。</p>

<h2>一元线性回归(Linear regression with one variable)</h2>

<p>Regression Problem : Predict real-valued output</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/8.png' width='' height='' title='1-1 算法运行的过程'><span class='caption-text'>1-1 算法运行的过程</span></span></p>

<p>最关键的在于如何描述hypothesis。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/1.png' width='' height='' title='1-2 一元线性回归中的hypothesis函数'><span class='caption-text'>1-2 一元线性回归中的hypothesis函数</span></span></p>

<p>那么应该如何选取参数呢？直觉告诉我们这个直线需要尽可能的拟合我们的数据集。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/9.png' width='' height='' title='1-3 线性回归的目标函数'><span class='caption-text'>1-3 线性回归的目标函数</span></span></p>

<p>通过下面的cost function 来评估参数的好坏。算法的目标也很清晰，让函数越小越好。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/12.png' width='' height='' title='1-4 cost function'><span class='caption-text'>1-4 cost function</span></span></p>

<p>那个这个cost function 到底是个什么样子呢？</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/2.png' width='' height='' title='1-5 图形化的cost function'><span class='caption-text'>1-5 图形化的cost function</span></span></p>

<p>当然这个图还是看起来比较麻烦，Andrew 用了更为简单绘制的图来表示（有点类似等高线）。
相同的圆圈上，有着相同的cost function value。这里可以看到和上面的图一样，有一个极值。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/7.png' width='' height='' title='1-6 一个比较差的选择'><span class='caption-text'>1-6 一个比较差的选择</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/18.png' width='' height='' title='1-7 一个很接近极值的选择'><span class='caption-text'>1-7 一个很接近极值的选择</span></span></p>

<h1>梯度下降 (Gradient descent)</h1>

<p>梯度下降，不仅仅是用于线性回归，也可以用在其他机器学习的场景下。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/3.png' width='' height='' title='1-8 梯度下降的思路（2个参数的情况）'><span class='caption-text'>1-8 梯度下降的思路（2个参数的情况）</span></span></p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/10.png' width='' height='' title='1-9 梯度下降函数图形（2个参数的情况）'><span class='caption-text'>1-9 梯度下降函数图形（2个参数的情况）</span></span></p>

<p>我们的目标是寻找这个图形中的最小值，也就是靠近蓝色的地方。直觉告诉我们，我们先随机一个点，然后沿着最大的坡度向下走最后就可以走到一个极值里。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/16.png' width='' height='' title='1-10 一条算法路径，全局最优'><span class='caption-text'>1-10 一条算法路径，全局最优</span></span></p>

<p>这个算法也有问题，随着第一个点的位置不同，我们可能找到一个局部最优的解，而不是全局最优。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/14.png' width='' height='' title='1-11 另一条算法路径，局部最优'><span class='caption-text'>1-11 另一条算法路径，局部最优</span></span></p>

<p>好在在很多实际问题中，我们遇到的情况要好很多，往往<strong>只有一个极值</strong>。</p>

<p>那么梯度下降的算法就可以简单的描述出来，分别计算2个维度的偏导数，直到函数收敛</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/5.png' width='' height='' title='1-12'><span class='caption-text'>1-12</span></span></p>

<p>通过分别计算偏导数,a 为learning rate，决定每一步的步长，太小函数收敛很慢，太大则可能无法找到极值，甚至函数无法收敛。</p>

<p>这里Andrew 着重指出了一个叫做同步更新的概念</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/11.png' width='' height='' title='1-13'><span class='caption-text'>1-13</span></span></p>

<p>如果不同步更新，最后也可以得到极致，但是Andrew 更推荐计算完成所有的参数之后，再一起同步更新。</p>

<h2>梯度下降和一元线性回归</h2>

<p>将图1-4分别偏导后</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/15.png' width='' height='' title='1-14 算法公式'><span class='caption-text'>1-14 算法公式</span></span></p>

<h2>其他</h2>

<ol>
<li>根据上面的算法，如果我们的cost function 在一些地方不可导，那算法不就没法继续了？</li>
<li>有其他的方法，可以不去循环计算而是直接根据工具计算</li>
</ol>


<h2>梯度下降和一般化的线性回归</h2>

<p>很多时候我们不仅仅满足2个参数，决定事情的因素很多，我们需要更一般化的公式。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/4.png' width='' height='' title='1-15'><span class='caption-text'>1-15</span></span></p>

<p>算法</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/19.png' width='' height='' title='1-16'><span class='caption-text'>1-16</span></span></p>

<p>分别求偏导后</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/17.png' width='' height='' title='1-17'><span class='caption-text'>1-17</span></span></p>

<h1>梯度下降生产环境中的一些技巧</h1>

<h2>Feature Scaling</h2>

<p>思路: 希望所有的feature在相同或是类似的范围之内，这样梯度下降会更快收敛。</p>

<p>下图是feature的范围不在一起的运算过程，可以看出来不是圆形，2个维度调整的步长不一样，导致很多反复</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/21.png' width='' height='' title='1-18 红色箭头表示算法的一次迭代'><span class='caption-text'>1-18 红色箭头表示算法的一次迭代</span></span></p>

<p>下图则是调整过的feature，好了很多</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/23.png' width='' height='' title='1-19 红色箭头表示算法的一次迭代'><span class='caption-text'>1-19 红色箭头表示算法的一次迭代</span></span></p>

<p>更一般的，Andrew 推荐每一个feature放在[-1, 1]区间范围内</p>

<h2>Learning Rate</h2>

<p>说到Learning Rate 就不能不提收敛(convergence)。一般应该定义多大的阀值来判断是否收敛呢？</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/6.png' width='' height='' title='1-20 Andrew 并不推荐使用一个阀值来判断是否收敛'><span class='caption-text'>1-20 Andrew 并不推荐使用一个阀值来判断是否收敛</span></span></p>

<p>Andrew 更推荐用图表的形式，因为这个不仅仅可以看到是否马上收敛，而且还能看到算法是否运行正常，是不是一些参数的问题，导致算法无法收敛。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/20.png' width='' height='' title='1-21'><span class='caption-text'>1-21</span></span></p>

<p>下图是2个出了问题的J函数，通常来说是Learning Rate 过大。</p>

<p><span class='caption-wrapper'><img class='caption' src='/images/ml/25.png' width='' height='' title='1-22 一些过大的Learning Rate 导致的图形'><span class='caption-text'>1-22 一些过大的Learning Rate 导致的图形</span></span></p>

<p>最后Andrew 还提供了一些practice的Learning Rate 选取方法，比如一些0.001, 0.003, 0.01, 0.03, 0.1, ...</p>

<h2>参考</h2>

<p><a href="https://class.coursera.org/ml-006">Coursera 《Machine Learning》 Stanford Andrew Ng</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Stanford 算法课 part 2 Scheduling]]></title>
    <link href="http://studentdeng.github.com/blog/2013/09/16/algo-class-02-scheduling/"/>
    <updated>2013-09-16T20:46:00+08:00</updated>
    <id>http://studentdeng.github.com/blog/2013/09/16/algo-class-02-scheduling</id>
    <content type="html"><![CDATA[<p>当一个人每天都有做不完的事情时，不知道大家又没有这种感觉。各种各样的事情，各种deadline，甚至是一些无关紧要的琐碎事情也会不断的在大脑中被回想，而且还是反复的，没有任何理由的会冒出来。对像我这样非常懒惰的人来说，这就是一个非常致命的打击，因为我需要中断当前的事情，这会让我多消耗不少脑力。。。</p>

<p>这应该算是时间管理中的一部分，如何给自己安排任务。人的大脑和电脑的工作模式还是有一些类似的，比如如果一直不断的记录一大堆事情而不是执行，那么就会影响到他们的工作效率。同样，如果重要的事情不作，最后也会不断的在大脑里面被回想，而造成效率降低。</p>

<p>在我看来，解决这些回想问题最关键的事情就是把这些东西搞定，这样才能把大脑清空，然后去做那些更加有意义的事情。</p>

<p>最近再跟algorithm part2里面正好有一个scheduling application的东西，挺有意思的。这里记录一下。</p>

<h1>问题描述</h1>

<h2>Setup</h2>

<p>scheduling 的基本模型是有一个shared resource，比如CPU。但是有许多jobs，比如很多线程，需要使用CPU才能运行。</p>

<h2>Question</h2>

<p>我们应该如何调度这个jobs的顺序，哪一个job优先于其他job执行，从而让整个计划执行时间最少。</p>

<h2>Assume</h2>

<p>为了更清楚的定义数学模型，每一个job有2个维度。</p>

<ul>
<li>weight 重要性</li>
<li>length 时间</li>
</ul>


<h2>Defintion</h2>

<p>Completion time。 第J个任务的完成时间（Completion time）Cj 是 （任务J之前的等待时间 + 任务J的length）* 任务J的weight</p>

<h1>思路</h1>

<p>这里根据直觉可以很明显的知道 我们需要把重要的事情放在前面，把时间短的事情放置在前面，这样可以很快的打一个勾勾，清空大脑中的这个任务。也就是说Cj 和 Wj 正相关，和 Lj负相关。但是总是有一些事情让人欲罢不能，就是那些很重要，而且做起来还比较费劲，时间花费长的事情，和那些不重要但是时间花费短的事情。真实世界的事情太复杂，还是回到我们这个简单的模型中。</p>

<p>一种常见的思维是设法将问题转换成之前我们已经解决了的思路之中，所以很容易想到我们需要找到单位长度中最重要的事情先做。因为我们的这个模型的任务调度不是抢占式的么。</p>

<p>将上面的单位时间的含义翻译过来就是 wj / lj。 也就是我们的调度程序将按照w / l的值，从大到小排列
<img src="https://raw.github.com/studentdeng/studentdeng.github.com/master/images/algo_schema_01.png" alt="scheduling 1" /></p>

<p>这里我们假定任务i > j。这个图则表示这2个任务相连在一起，stuff表示之前的任务，more stuff表示之后的任务。</p>

<p><img src="https://raw.github.com/studentdeng/studentdeng.github.com/master/images/algo_schema_02.png" alt="scheduling 2" /></p>

<p>而这里，我们做一次任务i、j的交换，就像这样</p>

<p><img src="https://raw.github.com/studentdeng/studentdeng.github.com/master/images/algo_schema_03.png" alt="image" /></p>

<p>那个这2个scheduling中，i,j之前的任务的完成时间不会变，之后的任务完成时间也不会变。那么受到影响的只是任务i，j。</p>

<p><img src="https://raw.github.com/studentdeng/studentdeng.github.com/master/images/algo_schema_04.png" alt="image" /></p>

<p>那么这里显然scheduling 2 也就是后面的那张图的顺序比第一个要小。</p>

<p>然后我们可以想象最后的任务流程，就如果冒泡排序一样，一次次的比较把相对重要的数据一次次的放置到前面。</p>

<p>而当所有相邻的任务都按照这样的规则排列完毕后，得到的就是一个 wj / lj 的从大到小的序列。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[跳跃表 skip list]]></title>
    <link href="http://studentdeng.github.com/blog/2013/08/08/skip-list/"/>
    <updated>2013-08-08T09:51:00+08:00</updated>
    <id>http://studentdeng.github.com/blog/2013/08/08/skip-list</id>
    <content type="html"><![CDATA[<p>最近在学习<a href="http://redis.cn">redis</a>，这时才知道了<a href="http://en.wikipedia.org/wiki/Skip_list">skip list</a>，结合<a href="http://v.163.com/movie/2010/12/7/S/M6UTT5U0I_M6V2TTJ7S.html">Mit 算法导论 lecture 12</a>，在奋斗了2个早上的时间后有了下面的东东。</p>

<p>对于我们熟悉的binary search来说，我们需要能够做到random access才行。但是在普通的link这种数据结构中却不能做到。而这种情况下我们有很多类似的工具比如heap，tree，b tree，red－black tree。等等类似的都是来自AVL的变种。但是说实话，这些东东，的确是挺难实现的，需要做各种的旋转啊，调整啊，来保持平衡。特别是red－black tree。而这时的skip list 就为我们提供了一个很好的思路。</p>

<h1>introduction</h1>

<p>让我们先从简单的开始</p>

<p><img src="http://studentdeng.github.io/images/skip_list1.png" alt="image" /></p>

<p>如果最下面的数字是已经排序好的数列，我们想要快速查找其中一项，而不是简单的便利。我们可以增加一个link，也就是上面的一条，来让我们能够“跳过”一些元素，也就是减少一些不必要的比较。</p>

<p>那么在2条时，我们的访问程度是多少呢？L2 + L1 / L2, 也就是第二条link的个数+ 每一个小端个数，这个是最差情况。显然，让这个不等式和最小，需要 L2 = L1 / L2。 显然L1是一个定值。这里设为N,那么，2条link下，我们的查找复杂度是 2 * √n</p>

<p>如何再优化呢？这个思路很简单，就是在L2上面再构建一个link L3. 整个时间也就是 L3 ＋ L2 ／ L3 ＋ L1 ／ L2 ，根据不等式性质，他们的和最小时，也就是 L3 ＝ L2 ／ L3 ＝ L1 ／ L2。当L1 ＝ N时，他们的和时 3 * 立方根（N）</p>

<p><img src="http://studentdeng.github.io/images/skip_list_l3.png" alt="image" /></p>

<p>当第k层时， 我们的时间则是 k * k次方跟（N）</p>

<p>当k = lgN 时，我们的时间为 lgN * lg 次方跟（N），根据对数的换底公式，我们可以得出 时间是 2lgN. 哈，我们现在已经降到O(lgN).我们满足了。</p>

<p>这时我们可以想象一下，这个skip list的结构，其实就是一个binary tree。我们通过最上面的一层访问类似跟节点的情况，然后一层层link 相当于tree的孩子节点，整个比较过程和binary search 非常的相似。</p>

<h1>insert</h1>

<p>对于这些结构来说，搞定search不是难点，插入和删除则是最麻烦的东西。这里我们可以自己思考一下，为了保证我们的link的结构足够完美，可能需要记录没一段的个数，然后我们可能有一些节点要上几层或是下几层。但是这个其实，本质上和那些avl树又一样了。skip list则是基于一种随机的策略来决定这些节点。其实我们可以思考一下，最完美的分法就是和binary tree一样的，所以这种2倍数的关系就可以用抛硬币的方式来决定。</p>

<p>这里为了程序时间方便，我们创建一个无穷小的节点作为我们的其实节点，这样，我们所有的开始都是从最左边。</p>

<p><img src="http://studentdeng.github.io/images/skip_list_01.png" alt="image" /></p>

<p>我们插入一个元素30，这时我们可以判断一下这个新的元素是否需要“升级”，这里我扔了一下，反面，不用升级了。</p>

<p><img src="http://studentdeng.github.io/images/skip_list_02.png" alt="image" /></p>

<p>这里我们插入一个15，我扔了一下，反面。不用升级</p>

<p><img src="http://studentdeng.github.io/images/skip_list_03.png" alt="image" /></p>

<p>这里我们插入一个20，我扔了一个正面，又扔了一个正面，额，好吧第三次终于是反面了。</p>

<p><img src="http://studentdeng.github.io/images/skip_list_04.png" alt="image" /></p>

<p>这里涉及到了一点点的随机算法的证明，这些东西实在是让人烦躁。主要还是大学时候的概率学得就不咋地，现在也都忘了。从最直观的来看，就是一层层升级的概率会越来越低，在随机算法足够独立和大量的数目上来看，不难形成这样子的一个类似tree的结构。</p>

<h1>delete</h1>

<p>删除这里的操作简直就是blazingly simple,因为我们整个list layer 都是建立在随机上的，删除则是直接删除就好了</p>

<p>我在看到这里，基本已经受不了要吐槽了。实现这个也太简单了，相对red-black tree这种东西。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[stanford 算法课上 Kosaraju algorithm]]></title>
    <link href="http://studentdeng.github.com/blog/2013/08/04/algorithms-class4/"/>
    <updated>2013-08-04T23:32:00+08:00</updated>
    <id>http://studentdeng.github.com/blog/2013/08/04/algorithms-class4</id>
    <content type="html"><![CDATA[<p>强连通图的应用场景我就不在这里赘述了。其中<a href="http://en.wikipedia.org/wiki/Kosaraju's_algorithm">Kosaraju</a>是最常见的一种。</p>

<p>这个也是Stanford 算法课<a href="https://class.coursera.org/algo-004/quiz/attempt?quiz_id=57">弟四周的作业</a>，现在看来是最难的一道题。那么这里我就给一个我自己的实现了。</p>

<p>这个作业的难度就在于他的输入是一个相当大的数据，处理不好，很容易溢出。那份大数据，我没有留在这里，感兴趣的同学可以自己下载。70多M，实在不适合放在github上面。</p>

<p><a href="https://github.com/studentdeng/algorithms_class">source_code</a></p>

<p>由于是xcode的环境，在g++下是过不去的。。。</p>
]]></content>
  </entry>
  
</feed>
